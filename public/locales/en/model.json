{
  "configuration": "Configuration",
  "model": "Model",
  "token": {
    "label": "Max Token",
    "description": "The maximum number of tokens to generate in the chat completion."
  },
  "default": "Default",
  "temperature": {
    "label": "Temperature",
    "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random. (Default: 1)"
  },
  "presencePenalty": {
    "label": "Presence Penalty",
    "description": "Number between -2.0 and 2.0. Positive values penalise new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. (Default: 0)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Number between 0 and 1. An alternative to sampling with temperature, called nucleus sampling. (Default: 1)"

  },
  "frequencyPenalty": {
    "label": "Frequency Penalty",
    "description": "Number between -2.0 and 2.0. Positive values penalise new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. (Default: 0)"
  },
  "defaultChatConfig": "Default Chat Config",
  "defaultSystemMessage": "Default System Message",
  "resetToDefault": "Reset To Default"
}
